name: WikiDocumentation

# Example usage:
#   jobs:
#     run-docs-pipeline:
#       uses: org/repo/.github/workflows/WikiDocumentation.yaml@main
#       with:
#         app_name: "wiki-upload-test "

on:
  workflow_call:
    inputs:
      app_name:
        description: "Application name to be used when syncing documentation."
        required: true
        type: string
      base_path:
        description: "Base folder path in the Wiki assets (e.g. 'Pedro/Aplica√ß√µes')."
        required: true
        type: string
    secrets:
      WIKI_API_TOKEN:
        description: "API token for Wiki.js GraphQL access"
        required: true

permissions:
  contents: read
  actions: write

env:
  WIKI_URL: "http://wiki.franquinho.info/graphql"
  #WIKI_URL: "http://cyberwiki.internal.ctt.pt/graphql"
  #WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN_WIKILAB2  }}
  WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN }}
  WIKI_LOCALE: "en"
  BASE_PATH: ${{ inputs.base_path }}
  REPO_NAME: ${{ inputs.app_name != '' && inputs.app_name || github.event.repository.name }}

jobs:
  # --- Phase 1: Setup ---
  setup-env:
    name: "Setup Environment"
    runs-on: ubuntu-latest
    outputs:
      # removed BASE_PATH_SAFE (unused)
      ASSET_PATH_SAFE: ${{ steps.export.outputs.ASSET_PATH_SAFE }}
    steps:
      - name: Compute ASSET_PATH_SAFE
        id: export
        run: |
          set -euo pipefail
          # removed BASE_PATH_SAFE (unused)
          raw_base="${BASE_PATH:-}"
          raw_app="${REPO_NAME:-}"

          sanitize_segment() {
            local segment="$1"
            segment=$(echo "$segment" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null)
            segment=$(echo "$segment" | tr '[:upper:]' '[:lower:]')
            segment=$(echo "$segment" | sed 's/[^a-z0-9._-]/-/g')
            segment=$(echo "$segment" | sed 's/-\{2,\}/-/g')
            segment=$(echo "$segment" | sed 's/^-//; s/-$//')
            echo "$segment"
          }

          asset_path="${raw_base}/${raw_app}"
          asset_safe=""
          oldIFS="$IFS"
          IFS='/'
          read -r -a segments <<< "$asset_path"
          IFS="$oldIFS"
          for segment in "${segments[@]}"; do
            [ -z "$segment" ] && continue
            clean_segment=$(sanitize_segment "$segment")
            [ -z "$clean_segment" ] && continue
            asset_safe="${asset_safe}/${clean_segment}"
          done
          asset_safe="${asset_safe#/}"
          asset_safe="${asset_safe%/}"
          echo "ASSET_PATH_SAFE=$asset_safe"
          echo "ASSET_PATH_SAFE=$asset_safe" >> "$GITHUB_OUTPUT"

      - name: Save shared Bash functions
        run: |
          mkdir -p scripts
          cat > scripts/fix_paths.sh <<'EOF'
          #!/usr/bin/env bash
          set -euo pipefail

          fix_image_paths() {
            local prefix="/${ASSET_PATH_SAFE}/documentation/"

            sed -E "
              # 1Ô∏è‚É£ Corrige links tipo ./documentation/...
              s#\]\((<?)(\./)?documentation/#](\1${prefix}#g;

              # 2Ô∏è‚É£ Corrige refer√™ncia [id]: ./documentation/...
              s#^(\[[^\]]+\]:[[:space:]]*)(<?)(\./)?documentation/#\1\2${prefix}#g;

              # 3Ô∏è‚É£ Corrige imagens simples sem 'documentation/'
              # Ex: ![alt text](imagem.png)
              s#(!\[[^]]*\]\()[[:space:]]*([A-Za-z0-9._-]+\.(png|jpg|jpeg|gif|svg|webp|drawio|pdf))[[:space:]]*\)#\1${prefix}\2)#g;
            "
          }

          EOF
      - name: Upload helper script for later jobs
        uses: actions/upload-artifact@v4
        with:
          name: fix_paths_script
          path: scripts/fix_paths.sh

  # --- Phase 2: Wiki Discovery ---
  check-pages-folders:
    name: "Check if Pages and Folders are created"
    runs-on: ubuntu-latest
    needs: [setup-env]
    outputs:
      has_repo_path: ${{ steps.check.outputs.has_repo_path }}
      has_documentation_folder: ${{ steps.check.outputs.has_documentation_folder }}
      has_readme_page: ${{ steps.check.outputs.has_readme_page }}
      has_changelog_page: ${{ steps.check.outputs.has_changelog_page }}
      repo_path: ${{ steps.check.outputs.repo_path }}
      missing_files: ${{ steps.check.outputs.missing_files }}
      docs_files: ${{ steps.check.outputs.docs_files }}
      missing_count: ${{ steps.check.outputs.missing_count }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Query Wiki.js and verify structure
        id: check
        run: |
          set -euo pipefail

          # --- Paths ---
          REPO_SLUG="$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null \
            | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')"

          REPO_PATH="$(echo "${BASE_PATH}/${REPO_SLUG}" | sed 's#//*#/#g; s#/$##')"
          DOC_PREFIX="${REPO_PATH}/documentation/"
          README_PATH="${REPO_PATH}/readme"
          CHANGELOG_PATH="${REPO_PATH}/changelog"

          echo "üîç Checking Wiki structure for ${REPO_PATH}"

          # --- Query restricted to tagged pages ---
          QUERY=$(cat <<'GRAPHQL'
          query ($locale: String!) {
            pages {
              list(locale: $locale, tags: ["gh-sync"]) {
                id
                path
                title
                tags
              }
            }
          }
          GRAPHQL
          )

          JSON_PAYLOAD=$(jq -n --arg q "$QUERY" --arg loc "$WIKI_LOCALE" \
            '{query: $q, variables: { locale: $loc }}')

          RESULT=$(curl -sS -X POST "$WIKI_URL" \
            -H "Authorization: Bearer $WIKI_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$JSON_PAYLOAD")

          # --- Verifica√ß√£o de erros ---
          if jq -e '.errors' >/dev/null 2>&1 <<<"$RESULT"; then
            echo "‚ùå GraphQL error:"; echo "$RESULT" | jq '.errors'; exit 1
          fi

          # --- Flags principais ---
          HAS_REPO_PATH=$(echo "$RESULT" | jq --arg path "$REPO_PATH" \
            '[.data.pages.list[] | select(.path | startswith($path))] | length > 0')
          HAS_DOCUMENTATION_FOLDER=$(echo "$RESULT" | jq --arg path "$DOC_PREFIX" \
            '[.data.pages.list[] | select(.path | startswith($path))] | length > 0')
          HAS_README_PAGE=$(echo "$RESULT" | jq --arg path "$README_PATH" \
            '[.data.pages.list[] | select(.path == $path)] | length > 0')
          HAS_CHANGELOG_PAGE=$(echo "$RESULT" | jq --arg path "$CHANGELOG_PATH" \
            '[.data.pages.list[] | select(.path == $path)] | length > 0')

          # --- Ficheiros locais ---
          DOC_FILES=($(find documentation -type f -name "*.md" -exec basename {} .md \;))
          MISSING_FILES=()

          for FILE in "${DOC_FILES[@]}"; do
            FILE_PATH="${DOC_PREFIX}${FILE}"
            EXISTS=$(echo "$RESULT" | jq --arg path "$FILE_PATH" \
              '[.data.pages.list[] | select(.path == $path)] | length > 0')
            [ "$EXISTS" != "true" ] && MISSING_FILES+=("$FILE_PATH")
          done

          # --- Outputs ---
          echo "has_repo_path=$HAS_REPO_PATH" >> "$GITHUB_OUTPUT"
          echo "has_documentation_folder=$HAS_DOCUMENTATION_FOLDER" >> "$GITHUB_OUTPUT"
          echo "has_readme_page=$HAS_README_PAGE" >> "$GITHUB_OUTPUT"
          echo "has_changelog_page=$HAS_CHANGELOG_PAGE" >> "$GITHUB_OUTPUT"
          echo "repo_path=$REPO_PATH" >> "$GITHUB_OUTPUT"
          echo "missing_files=${MISSING_FILES[*]}" >> "$GITHUB_OUTPUT"
          echo "missing_count=${#MISSING_FILES[@]}" >> "$GITHUB_OUTPUT"
          echo "docs_files=${DOC_FILES[*]}" >> "$GITHUB_OUTPUT"

          echo "‚úÖ Wiki discovery complete."

  # --- Phase 3: Change Detection ---
  check-altered-files:
    name: "Check Changed Files"
    runs-on: ubuntu-latest
    needs: [setup-env]
    outputs:
      DOCS_CHANGED: ${{ steps.check.outputs.DOCS_CHANGED }}
      README_CHANGED: ${{ steps.check.outputs.README_CHANGED }}
      CHANGELOG_CHANGED: ${{ steps.check.outputs.CHANGELOG_CHANGED }}
      DOC_FILES_CHANGED: ${{ steps.check.outputs.DOC_FILES_CHANGED }}
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0   # precisamos do hist√≥rico completo para o git diff funcionar corretamente

      - name: Get changed files
        id: changed-files
        run: |
          set -euo pipefail
          BEFORE="${{ github.event.before }}"
          ZERO="0000000000000000000000000000000000000000"

          echo "üîç Determining changed files..."
          if [ -z "$BEFORE" ] || [ "$BEFORE" = "$ZERO" ]; then
            # Se for o primeiro commit ou run manual
            if git rev-parse HEAD^ >/dev/null 2>&1; then
              git diff --name-only --diff-filter=AM HEAD^ HEAD > changed_files.txt
            else
              git show --name-only --pretty="" --diff-filter=AM HEAD > changed_files.txt
            fi
          else
            # S√≥ compara o intervalo real
            git diff --name-only --diff-filter=AM "$BEFORE" ${{ github.sha }} > changed_files.txt
          fi

          echo "Changed files:"
          cat changed_files.txt || echo "(none)"
          echo "----"
          # Remove duplicados e linhas vazias
          awk 'NF && !a[$0]++' changed_files.txt > tmp && mv tmp changed_files.txt
          cat changed_files.txt

      - name: Check if documentation files changed
        id: check
        run: |
          set -euo pipefail

          DOC_FILES_CHANGED=$(grep -E '^documentation/.+\.md$' changed_files.txt || true)
          DOC_FILES_CHANGED=$(echo "$DOC_FILES_CHANGED" | tr -d '\r' | sort -u | tr '\n' ' ' | xargs)

          if [ -n "$DOC_FILES_CHANGED" ]; then
            echo "üìÑ Documentation files changed: $DOC_FILES_CHANGED"
            echo "DOCS_CHANGED=true" >> "$GITHUB_OUTPUT"
            echo "DOC_FILES_CHANGED=$DOC_FILES_CHANGED" >> "$GITHUB_OUTPUT"
          else
            echo "üìÑ No documentation files changed."
            echo "DOCS_CHANGED=false" >> "$GITHUB_OUTPUT"
            echo "DOC_FILES_CHANGED=" >> "$GITHUB_OUTPUT"
          fi

          if grep -qE '^README\.md$' changed_files.txt; then
            echo "üìò README.md changed."
            echo "README_CHANGED=true" >> "$GITHUB_OUTPUT"
          else
            echo "README_CHANGED=false" >> "$GITHUB_OUTPUT"
          fi

          if grep -qE '^CHANGELOG\.md$' changed_files.txt; then
            echo "üìò CHANGELOG.md changed."
            echo "CHANGELOG_CHANGED=true" >> "$GITHUB_OUTPUT"
          else
            echo "CHANGELOG_CHANGED=false" >> "$GITHUB_OUTPUT"
          fi

  
  # --- Phase 4: Content Creation ---
  create-pages:
    name: "Create Missing Wiki Pages"
    runs-on: ubuntu-latest
    needs: [setup-env, check-pages-folders]
    if: needs.check-pages-folders.outputs.has_repo_path == 'false' || needs.check-pages-folders.outputs.missing_count > 0

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download helper scripts
        uses: actions/download-artifact@v4
        with:
          name: fix_paths_script
          path: scripts

      - name: Prepare shared functions
        run: |
          set -euo pipefail
          mkdir -p scripts
          cat <<'EOF' > scripts/wiki_utils.sh
          #!/usr/bin/env bash
          set -euo pipefail
          source scripts/fix_paths.sh

          normalize_markdown_links() {
            sed -E \
              -e 's#\]\([[:space:]]*/documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md([?#][^)]*)\)#](./documentation/\1\2)#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md\)#](./documentation/\1)#g'
          }

          # Create a new page in Wiki.js with tag "gh-sync"
          create_wiki_page() {
            local src="$1"
            local path="$2"
            local title="$3"
            local content="$4"

            echo "üÜï Creating page: $path"

            local QUERY='mutation ($path: String!, $title: String!, $content: String!, $locale: String!) {
              pages {
                create(
                  path: $path,
                  title: $title,
                  description: "",
                  content: $content,
                  editor: "markdown",
                  isPrivate: false,
                  isPublished: true,
                  locale: $locale,
                  tags: ["gh-sync"]
                ) {
                  responseResult { succeeded message }
                  page { id path title tags { title } }
                }
              }
            }'

            local PAYLOAD
            PAYLOAD=$(jq -n \
              --arg q "$QUERY" \
              --arg path "$path" \
              --arg title "$title" \
              --arg content "$content" \
              --arg loc "$WIKI_LOCALE" \
              '{ query: $q, variables: { path: $path, title: $title, content: $content, locale: $loc } }')

            local RESULT
            RESULT=$(curl -sS -X POST "$WIKI_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$PAYLOAD")

            local OK
            OK=$(echo "$RESULT" | jq -r '.data.pages.create.responseResult.succeeded // empty')

            if [ "$OK" = "true" ]; then
              local NEW_ID
              NEW_ID=$(echo "$RESULT" | jq -r '.data.pages.create.page.id')
              echo "‚úÖ Created page \"$title\" (ID: $NEW_ID)"
            else
              echo "‚ö†Ô∏è Failed to create page \"$title\""
              echo "$RESULT" | jq '.errors // .data.pages.create.responseResult'
              exit 1
            fi
          }
          EOF
          chmod +x scripts/wiki_utils.sh

      - name: Create pages in Wiki.js
        run: |
          set -euo pipefail
          source scripts/wiki_utils.sh

          REPO_SLUG=$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')
          REPO_PATH="${BASE_PATH}/${REPO_SLUG}"
          DOC_PREFIX="${REPO_PATH}/documentation"
          echo "üìÇ Base path: $REPO_PATH"

          ASSET_PATH_SAFE="${{ needs.setup-env.outputs.ASSET_PATH_SAFE }}"
          export ASSET_PATH_SAFE

          # README
          if [ "${{ needs.check-pages-folders.outputs.has_readme_page }}" = "false" ] && [ -f README.md ]; then
            echo "ü™∂ Creating README..."
            CONTENT=$(tr -d '\r' < README.md | fix_image_paths | normalize_markdown_links)
            create_wiki_page "README.md" "${REPO_PATH}/readme" "README" "$CONTENT"
          fi

          # CHANGELOG
          if [ "${{ needs.check-pages-folders.outputs.has_changelog_page }}" = "false" ] && [ -f CHANGELOG.md ]; then
            echo "üìú Creating CHANGELOG..."
            CONTENT=$(tr -d '\r' < CHANGELOG.md | fix_image_paths | normalize_markdown_links)
            create_wiki_page "CHANGELOG.md" "${REPO_PATH}/changelog" "CHANGELOG" "$CONTENT"
          fi

          # Documentation files
          MISSING_FILES=(${{ needs.check-pages-folders.outputs.missing_files }})
          if [ "${#MISSING_FILES[@]}" -gt 0 ]; then
            echo "üìò Creating ${#MISSING_FILES[@]} missing documentation files..."
            for FILE in "${MISSING_FILES[@]}"; do
              SRC="documentation/$(basename "$FILE").md"
              [ ! -f "$SRC" ] && echo "‚ö†Ô∏è Missing local file $SRC" && continue
              TITLE=$(basename "$FILE")
              CONTENT=$(tr -d '\r' < "$SRC" | fix_image_paths | normalize_markdown_links)
              create_wiki_page "$SRC" "$FILE" "$TITLE" "$CONTENT"
            done
          else
            echo "‚úÖ No missing documentation files to create."
          fi


  # --- Phase 5: Content Updates ---
  update-pages:
    name: "Update Wiki Pages"
    runs-on: ubuntu-latest
    needs: [setup-env, check-pages-folders, check-altered-files]
    if: needs.check-pages-folders.outputs.has_repo_path == 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download helper scripts
        uses: actions/download-artifact@v4
        with:
          name: fix_paths_script
          path: scripts

      - name: Prepare shared functions
        run: |
          set -euo pipefail
          mkdir -p scripts
          cat <<'EOF' > scripts/wiki_utils.sh
          #!/usr/bin/env bash
          set -euo pipefail
          source scripts/fix_paths.sh

          normalize_markdown_links() {
            sed -E \
              -e 's#\]\([[:space:]]*/documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md([?#][^)]*)\)#](./documentation/\1\2)#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md\)#](./documentation/\1)#g'
          }

          # Get page ID scoped to current repo (with tag filter)
          get_page_id() {
            local rel_path="$1"
            local target="${rel_path#/}"

            local repo_slug
            repo_slug=$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null \
                        | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')

            local repo_prefix="${BASE_PATH}/${repo_slug}"
            repo_prefix="${repo_prefix//\/\//\/}"

            local QUERY='query ($locale: String!) {
              pages {
                list(locale: $locale, tags: ["gh-sync"]) { id title path }
              }
            }'

            local PAYLOAD
            PAYLOAD=$(jq -n --arg q "$QUERY" --arg loc "$WIKI_LOCALE" '{query:$q,variables:{locale:$loc}}')

            local RESULT
            RESULT=$(curl -sS -X POST "$WIKI_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$PAYLOAD")

            echo "$RESULT" | jq -r \
              --arg prefix "$repo_prefix" --arg target "$target" '
                .data.pages.list[]?
                | select(.path | startswith($prefix))
                | select(
                    (.path | ascii_downcase | gsub("√ß";"c") | gsub("√á";"c")) ==
                    ($target | ascii_downcase | gsub("√ß";"c") | gsub("√á";"c"))
                  )
                | .id
              ' | head -n 1
          }

          # Update keeping tag "gh-sync"
          update_wiki_page() {
            local id="$1"
            local content_file="$2"
            local title="$3"

            local CONTENT
            CONTENT=$(tr -d '\r' < "$content_file")

            local QUERY='mutation ($id: Int!, $content: String!, $locale: String!) {
              pages {
                update(
                  id: $id,
                  description: "",
                  content: $content,
                  editor: "markdown",
                  isPrivate: false,
                  isPublished: true,
                  locale: $locale,
                  tags: ["gh-sync"]
                ) {
                  responseResult { succeeded message }
                  page { id path updatedAt tags { title } }
                }
              }
            }'

            local JSON_PAYLOAD
            JSON_PAYLOAD=$(jq -n \
              --arg q "$QUERY" \
              --arg id "$id" \
              --arg content "$CONTENT" \
              --arg loc "$WIKI_LOCALE" \
              '{query: $q, variables: {id: ($id | tonumber), content: $content, locale: $loc}}')

            local RESULT
            RESULT=$(curl -sS -X POST "$WIKI_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$JSON_PAYLOAD")

            local OK
            OK=$(echo "$RESULT" | jq -r '.data.pages.update.responseResult.succeeded // empty')

            if [ "$OK" = "true" ]; then
              echo "‚úÖ $title updated (ID $id)"
            else
              echo "‚ö†Ô∏è  Failed to update $title (ID $id)"
              echo "$RESULT" | jq '.errors // .data.pages.update.responseResult'
              exit 1
            fi
          }
          EOF
          chmod +x scripts/wiki_utils.sh

      - name: Update changed pages
        run: |
          set -euo pipefail
          export ASSET_PATH_SAFE="${{ needs.setup-env.outputs.ASSET_PATH_SAFE }}" || true
          : "${ASSET_PATH_SAFE:=default}"
          source scripts/wiki_utils.sh

          REPO_SLUG=$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')
          REPO_PATH="${BASE_PATH}/${REPO_SLUG}"

          RAW_MISSING="${{ needs.check-pages-folders.outputs.missing_files }}"
          IFS=' ' read -r -a MISSING_FILES <<< "$(echo "$RAW_MISSING" | tr -s ' ')"

          RAW_CHANGED="${{ needs.check-altered-files.outputs.DOC_FILES_CHANGED }}"
          IFS=' ' read -r -a CHANGED_DOCS <<< "$(echo "$RAW_CHANGED" | tr -s ' ')"

          DOCS_CHANGED="${{ needs.check-altered-files.outputs.DOCS_CHANGED }}"
          README_CHANGED="${{ needs.check-altered-files.outputs.README_CHANGED }}"
          CHANGELOG_CHANGED="${{ needs.check-altered-files.outputs.CHANGELOG_CHANGED }}"

          update_if_changed() {
            local file="$1"
            local title="$2"
            local subpath="$3"
            local changed="$4"

            if [ "$changed" = "true" ] && [ -f "$file" ]; then
              local path="${REPO_PATH}/${subpath}"

              for MISS in "${MISSING_FILES[@]}"; do
                [[ "$MISS" == *"${subpath}"* ]] && return
              done

              PAGE_ID=$(get_page_id "$path")
              [ -z "$PAGE_ID" ] && return

              local CONTENT
              CONTENT=$(tr -d '\r' < "$file" | fix_image_paths | normalize_markdown_links)
              SHORT_SHA="${GITHUB_SHA:-unknown}"
              CONTENT=$(printf '%s\n\n<!-- synced:%s -->\n' "$CONTENT" "${SHORT_SHA:0:7}")
              local CONTENT_FILE
              CONTENT_FILE="$(mktemp)"
              printf '%s' "$CONTENT" > "$CONTENT_FILE"
              update_wiki_page "$PAGE_ID" "$CONTENT_FILE" "$title"
            fi
          }

          update_if_changed "README.md" "README" "readme" "$README_CHANGED"
          update_if_changed "CHANGELOG.md" "CHANGELOG" "changelog" "$CHANGELOG_CHANGED"

          if [ "$DOCS_CHANGED" = "true" ]; then
            for FILE in "${CHANGED_DOCS[@]}"; do
              [ -z "$FILE" ] && continue
              DOC_TITLE=$(basename "$FILE" .md)
              DOC_PATH="${REPO_PATH}/documentation/${DOC_TITLE}"
              PAGE_ID=$(get_page_id "$DOC_PATH")
              [ -z "$PAGE_ID" ] && continue
              local CONTENT
              CONTENT=$(tr -d '\r' < "documentation/${DOC_TITLE}.md" | fix_image_paths | normalize_markdown_links)
              SHORT_SHA="${GITHUB_SHA:-unknown}"
              CONTENT=$(printf '%s\n\n<!-- synced:%s -->\n' "$CONTENT" "${SHORT_SHA:0:7}")
              local CONTENT_FILE
              CONTENT_FILE="$(mktemp)"
              printf '%s' "$CONTENT" > "$CONTENT_FILE"
              update_wiki_page "$PAGE_ID" "$CONTENT_FILE" "$DOC_TITLE"
            done
          else
            echo "‚ÑπÔ∏è No documentation files changed."
          fi


  # --- Phase 6: Asset Upload (always incremental) ---
  upload-docs-media:
    name: "Upload or Update Documentation Media"
    runs-on: ubuntu-latest
    needs: [setup-env, check-pages-folders, check-altered-files]
    if: always()
    env:
      ASSET_PATH_SAFE: ${{ needs.setup-env.outputs.ASSET_PATH_SAFE }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Detect Added or Modified Media Files
        id: detect
        run: |
          set -euo pipefail

          echo "üîç Detecting new or modified media files in documentation/"

          # Garante que temos hist√≥rico completo
          git fetch --prune --unshallow || git fetch --all
          git fetch origin main || true

          CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)
          echo "Current branch: $CURRENT_BRANCH"

          # Determina o commit base
          if git rev-parse origin/main >/dev/null 2>&1; then
            BASE_REF=$(git merge-base HEAD origin/main)
          else
            BASE_REF=$(git rev-list --max-parents=0 HEAD | head -n1)
          fi

          echo "Comparing against base commit: $BASE_REF"
          echo "Changed files since base:"

          # Mostra tudo o que mudou em documentation/
          git diff --name-only "$BASE_REF" HEAD -- 'documentation/' || true

          # Filtra apenas media (exclui .md)
          CHANGED_MEDIA=$(git diff --name-only "$BASE_REF" HEAD -- 'documentation/' | grep -v '\.md$' || true)

          if [ -z "$CHANGED_MEDIA" ]; then
            echo "‚ÑπÔ∏è  No new or modified media files detected."
            echo "MEDIA_FILES=" >> "$GITHUB_ENV"
          else
            echo "üì∏ Media files to upload/update:"
            echo "$CHANGED_MEDIA"
            echo "MEDIA_FILES=$(echo "$CHANGED_MEDIA" | paste -sd ';' -)" >> "$GITHUB_ENV"
          fi


      - name: Ensure asset folders exist
        if: env.MEDIA_FILES != ''
        env:
          WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN }}
        run: |
          set -euo pipefail
          APP_PATH="${ASSET_PATH_SAFE}/documentation"
          BASE_URL="${WIKI_URL%/graphql}"
          GRAPHQL_URL="${BASE_URL}/graphql"
          UPLOAD_URL="${BASE_URL}/u"

          slugify() {
            echo "$1" | tr '[:upper:]' '[:lower:]' |
              sed -E 's/[[:space:]]+/-/g; s/[^a-z0-9._-]+/-/g; s/-{2,}/-/g; s/^-+|-+$//g'
          }

          get_folder_id() {
            local parent="$1"; local name="$2"
            [[ -z "$parent" ]] && parent=0
            local slug; slug=$(slugify "$name")
            local q="query { assets { folders(parentFolderId: $parent) { id name slug } } }"
            local r=$(curl -s -X POST "$GRAPHQL_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$(jq -n --arg q "$q" '{query:$q}')")
            echo "$r" | jq -r --arg s "$slug" --arg n "$name" \
              '.data.assets.folders[]? | select((.slug==$s) or ((.name|ascii_downcase)==($n|ascii_downcase))) | .id' | head -n1
          }

          create_folder() {
            local parent="$1"; local name="$2"
            [[ -z "$parent" ]] && parent=0
            local slug; slug=$(slugify "$name")
            local m="mutation { assets { createFolder(parentFolderId: $parent, slug:\"$slug\", name:\"$name\") { responseResult { succeeded message } } } }"
            local r=$(curl -s -X POST "$GRAPHQL_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$(jq -n --arg q "$m" '{query:$q}')")
            echo "$r" | jq .
          }

          echo "üìÇ Ensuring folder path: $APP_PATH"
          IFS='/' read -ra PARTS <<< "$APP_PATH"
          CURRENT_PARENT=""

          for PART in "${PARTS[@]}"; do
            [ -z "$PART" ] && continue
            ID=$(get_folder_id "$CURRENT_PARENT" "$PART" | head -n1 || true)
            if [[ -z "$ID" || "$ID" == "null" ]]; then
              echo "Creating '$PART'..."
              create_folder "$CURRENT_PARENT" "$PART"
              ID=$(get_folder_id "$CURRENT_PARENT" "$PART" | head -n1 || true)
            fi
            echo "‚úì '$PART' id=$ID"
            CURRENT_PARENT="$ID"
          done

          echo "DOC_ROOT_FOLDER_ID=$CURRENT_PARENT" >> "$GITHUB_ENV"
          echo "UPLOAD_URL=$UPLOAD_URL" >> "$GITHUB_ENV"

      - name: Upload new or changed media
        if: env.MEDIA_FILES != ''
        env:
          WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN }}
        run: |
          set -euo pipefail
          FAILED=0
          IFS=';' read -ra FILES <<< "$MEDIA_FILES"

          for filepath in "${FILES[@]}"; do
            [ -z "$filepath" ] && continue
            if [ ! -f "$filepath" ]; then
              echo "‚ö†Ô∏è  Missing file: $filepath"
              continue
            fi
            echo "‚¨ÜÔ∏è Uploading or replacing '$filepath'..."
            RESPONSE=$(curl -s -w "\n%{http_code}" -X POST "$UPLOAD_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -F "mediaUpload={\"folderId\":${DOC_ROOT_FOLDER_ID}}" \
              -F "mediaUpload=@$filepath")
            CODE=$(echo "$RESPONSE" | tail -n1)
            BODY=$(echo "$RESPONSE" | sed '$d')

            if [[ "$CODE" =~ ^2 ]]; then
              echo "‚úÖ Uploaded: $filepath"
            else
              echo "‚ùå Upload failed ($CODE): $filepath"
              echo "$BODY" | jq . || echo "$BODY"
              FAILED=$((FAILED+1))
            fi
          done

          [ "$FAILED" -gt 0 ] && exit 1
          echo "‚úÖ Media sync complete."
