name: WikiDocumentation

# Example usage:
#   jobs:
#     run-docs-pipeline:
#       uses: org/repo/.github/workflows/WikiDocumentation.yaml@main
#       with:
#         app_name: "wiki-upload-test "

on:
  workflow_call:
    inputs:
      app_name:
        description: "Application name to be used when syncing documentation."
        required: true
        type: string
      base_path:
        description: "Base folder path in the Wiki assets (e.g. 'Pedro/Aplica√ß√µes')."
        required: true
        type: string
    secrets:
      WIKI_API_TOKEN:
        description: "API token for Wiki.js GraphQL access"
        required: true

permissions:
  contents: read
  actions: write

env:
  WIKI_URL: "http://wiki.franquinho.info/graphql"
  #WIKI_URL: "http://cyberwiki.internal.ctt.pt/graphql"
  #WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN_WIKILAB2  }}
  WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN }}
  WIKI_LOCALE: "en"
  BASE_PATH: ${{ inputs.base_path }}
  REPO_NAME: ${{ inputs.app_name != '' && inputs.app_name || github.event.repository.name }}

jobs:
  # --- Phase 1: Setup ---
  setup-env:
    name: "Setup Environment"
    runs-on: ubuntu-latest
    outputs:
      # removed BASE_PATH_SAFE (unused)
      ASSET_PATH_SAFE: ${{ steps.export.outputs.ASSET_PATH_SAFE }}
    steps:
      - name: Compute ASSET_PATH_SAFE
        id: export
        run: |
          set -euo pipefail
          # removed BASE_PATH_SAFE (unused)
          raw_base="${BASE_PATH:-}"
          raw_app="${REPO_NAME:-}"

          sanitize_segment() {
            local segment="$1"
            segment=$(echo "$segment" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null)
            segment=$(echo "$segment" | tr '[:upper:]' '[:lower:]')
            segment=$(echo "$segment" | sed 's/[^a-z0-9._-]/-/g')
            segment=$(echo "$segment" | sed 's/-\{2,\}/-/g')
            segment=$(echo "$segment" | sed 's/^-//; s/-$//')
            echo "$segment"
          }

          asset_path="${raw_base}/${raw_app}"
          asset_safe=""
          oldIFS="$IFS"
          IFS='/'
          read -r -a segments <<< "$asset_path"
          IFS="$oldIFS"
          for segment in "${segments[@]}"; do
            [ -z "$segment" ] && continue
            clean_segment=$(sanitize_segment "$segment")
            [ -z "$clean_segment" ] && continue
            asset_safe="${asset_safe}/${clean_segment}"
          done
          asset_safe="${asset_safe#/}"
          asset_safe="${asset_safe%/}"
          echo "ASSET_PATH_SAFE=$asset_safe"
          echo "ASSET_PATH_SAFE=$asset_safe" >> "$GITHUB_OUTPUT"

      - name: Save shared Bash functions
        run: |
          mkdir -p scripts
          cat > scripts/fix_paths.sh <<'EOF'
          #!/usr/bin/env bash
          set -euo pipefail

          fix_image_paths() {
            local prefix="/${ASSET_PATH_SAFE}/documentation/"

            sed -E "
              # 1Ô∏è‚É£ Corrige links tipo ./documentation/...
              s#\]\((<?)(\./)?documentation/#](\1${prefix}#g;

              # 2Ô∏è‚É£ Corrige refer√™ncia [id]: ./documentation/...
              s#^(\[[^\]]+\]:[[:space:]]*)(<?)(\./)?documentation/#\1\2${prefix}#g;

              # 3Ô∏è‚É£ Corrige imagens simples sem 'documentation/'
              # Ex: ![alt text](imagem.png)
              s#(!\[[^]]*\]\()[[:space:]]*([A-Za-z0-9._-]+\.(png|jpg|jpeg|gif|svg|webp|drawio|pdf))[[:space:]]*\)#\1${prefix}\2)#g;
            "
          }

          EOF
      - name: Upload helper script for later jobs
        uses: actions/upload-artifact@v4
        with:
          name: fix_paths_script
          path: scripts/fix_paths.sh

  # --- Phase 2: Wiki Discovery ---
  check-pages-folders:
    name: "Check if Pages and Folders are created"
    runs-on: ubuntu-latest
    needs: [setup-env]
    outputs:
      has_repo_path: ${{ steps.check.outputs.has_repo_path }}
      has_documentation_folder: ${{ steps.check.outputs.has_documentation_folder }}
      has_readme_page: ${{ steps.check.outputs.has_readme_page }}
      has_changelog_page: ${{ steps.check.outputs.has_changelog_page }}
      repo_path: ${{ steps.check.outputs.repo_path }}
      missing_files: ${{ steps.check.outputs.missing_files }}
      docs_files: ${{ steps.check.outputs.docs_files }}
      missing_count: ${{ steps.check.outputs.missing_count }}
    
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Query Wiki.js and verify structure
        id: check
        run: |
          set -euo pipefail

          # Definir Paths
          REPO_PATH_LOG="$(echo "${BASE_PATH}/${REPO_NAME}" | sed 's#//*#/#g' | sed 's#/$##')"
          REPO_SLUG="$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')" # sanitized for Wiki.js path
          REPO_PATH="$(echo "${BASE_PATH}/${REPO_SLUG}" | sed 's#//*#/#g' | sed 's#/$##')" # sanitized for Wiki.js path
          DOC_PREFIX="$(echo "${REPO_PATH}/documentation/" | sed 's#//*#/#g')" # sanitized for Wiki.js path
          README_PATH="$(echo "${REPO_PATH}/readme" | sed 's#//*#/#g')" # sanitized for Wiki.js path
          CHANGELOG_PATH="$(echo "${REPO_PATH}/changelog" | sed 's#//*#/#g')" # sanitized for Wiki.js path

          echo "üîç Verifica√ß√µes:"
          echo "  ‚Ä¢ REPO_PATH      = $REPO_PATH_LOG"
          echo "  ‚Ä¢ DOC_PREFIX     = $DOC_PREFIX"
          echo

          QUERY=$(cat <<'GRAPHQL'
          query ($locale: String!) {
            pages {
              list(locale: $locale) {
                id
                path
                title
              }
            }
          }
          GRAPHQL
          )

          JSON_PAYLOAD=$(jq -n --arg q "$QUERY" --arg loc "$WIKI_LOCALE" '{query: $q, variables: { locale: $loc }}')

          echo "Querying Wiki.js for all pages..."

          RESULT=$(curl -sS -X POST "$WIKI_URL" \
            -H "Authorization: Bearer $WIKI_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$JSON_PAYLOAD" \
            | jq --arg prefix "$BASE_PATH" '{data:{pages:{list:[.data.pages.list[] | select(.path | startswith($prefix))]}}}')

          echo "Filtered Wiki pages under base path '$BASE_PATH':"
          echo "$RESULT" | jq -r '.data.pages.list[].path'


          # Verrificar se Existem erros no resultado
          ERR_CNT=$(echo "$RESULT" | jq '.errors | length // 0')
          if [ "$ERR_CNT" -gt 0 ]; then
            echo "‚ùå Erros na consulta:"
            echo "$RESULT" | jq '.errors'
            exit 1
          fi

          # Flags para verificar se os paths existem
          HAS_REPO_PATH=$(echo "$RESULT" | jq --arg path "$REPO_PATH" '[.data.pages.list[] | select(.path | startswith($path))] | length > 0')
          HAS_DOCUMENTATION_FOLDER=$(echo "$RESULT" | jq --arg path "$DOC_PREFIX" '[.data.pages.list[] | select(.path | startswith($path))] | length > 0')
          HAS_README_PAGE=$(echo "$RESULT" | jq --arg path "$README_PATH" '[.data.pages.list[] | select(.path == $path)] | length > 0')
          HAS_CHANGELOG_PAGE=$(echo "$RESULT" | jq --arg path "$CHANGELOG_PATH" '[.data.pages.list[] | select(.path == $path)] | length > 0')


          # List all .md files in documentation folder
          DOC_FILES=$(find documentation -type f -name "*.md" | sed 's#^documentation/##')
          DOC_FILES=($DOC_FILES)  # Convert to array

          # Remove the Extension from the filenames
          for i in "${!DOC_FILES[@]}"; do
            DOC_FILES[$i]=$(basename "${DOC_FILES[$i]}" .md)
          done

          # Verificar se esses arquivos existem na Wiki
          MISSING_FILES=()
          for FILE in "${DOC_FILES[@]}"; do
            FILE_PATH="${DOC_PREFIX}${FILE}"
            # echo "  ‚Ä¢ Checking if $FILE_PATH exists in Wiki..."
            EXISTS=$(echo "$RESULT" | jq --arg path "$FILE_PATH" '[.data.pages.list[] | select(.path == $path)] | length > 0')
            if [ "$EXISTS" != "true" ]; then
              MISSING_FILES+=("$FILE_PATH")
            fi
          done

          if [ ${#MISSING_FILES[@]} -gt 0 ]; then
            echo "  ‚Ä¢ Missing documentation files in Wiki:"
            for FILE in "${MISSING_FILES[@]}"; do
              echo "    - $FILE"
            done
          fi
          echo "‚úÖ Verifica√ß√µes conclu√≠das."

          # Set outputs
          echo "has_repo_path=$HAS_REPO_PATH" >> $GITHUB_OUTPUT
          echo "has_documentation_folder=$HAS_DOCUMENTATION_FOLDER" >> $GITHUB_OUTPUT
          echo "has_readme_page=$HAS_README_PAGE" >> $GITHUB_OUTPUT
          echo "has_changelog_page=$HAS_CHANGELOG_PAGE" >> $GITHUB_OUTPUT
          echo "repo_path=$REPO_PATH" >> $GITHUB_OUTPUT
          echo "missing_files=${MISSING_FILES[*]}" >> $GITHUB_OUTPUT
          echo "missing_count=${#MISSING_FILES[@]}" >> "$GITHUB_OUTPUT"
          echo "docs_files=${DOC_FILES[*]}" >> $GITHUB_OUTPUT

          # Echo dos outputs para debug
          echo "‚úÖ Outputs set:"
          echo "  ‚Ä¢ has_repo_path = $HAS_REPO_PATH"
          echo "  ‚Ä¢ has_documentation_folder = $HAS_DOCUMENTATION_FOLDER"
          echo "  ‚Ä¢ has_readme_page = $HAS_README_PAGE"
          echo "  ‚Ä¢ has_changelog_page = $HAS_CHANGELOG_PAGE"
          echo "  ‚Ä¢ missing_files = ${MISSING_FILES[*]}"
          echo "  ‚Ä¢ docs_files = ${DOC_FILES[*]}"

  # --- Phase 3: Change Detection ---
  check-altered-files:
    name: "Check Changed Files"
    runs-on: ubuntu-latest
    needs: [setup-env]
    outputs:
      docs_changed: ${{ steps.check.outputs.docs_changed }}
      readme_changed: ${{ steps.check.outputs.readme_changed }}
      changelog_changed: ${{ steps.check.outputs.changelog_changed }}
      doc_files_changed: ${{ steps.check.outputs.doc_files_changed }}
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Get changed files
        id: changed-files
        run: |
          # Fallback se for o primeiro commit ou trigger manual
          if [ -z "${{ github.event.before }}" ] || [ "${{ github.event.before }}" = "0000000000000000000000000000000000000000" ]; then
            git ls-files > changed_files.txt
          else
            git fetch origin ${{ github.event.before }}
            git diff --name-only --diff-filter=AM ${{ github.event.before }} ${{ github.sha }} > changed_files.txt
          fi
          cat changed_files.txt

      - name: Check if documentation files changed
        id: check
        run: |
          if grep -qE '^documentation/.+\.md$' changed_files.txt; then
            echo "Documentation files have changed."
            echo "docs_changed=true" >> "$GITHUB_OUTPUT"
            DOC_FILES_CHANGED=$(grep -E '^documentation/.+\.md$' changed_files.txt | tr -d '\r' | xargs)
            echo "Changed documentation files: $DOC_FILES_CHANGED"
            echo "DOC_FILES_CHANGED=$DOC_FILES_CHANGED" >> "$GITHUB_OUTPUT"
          else
            echo "docs_changed=false" >> "$GITHUB_OUTPUT"
          fi

          if grep -qE '^README\.md$' changed_files.txt; then
            echo "readme_changed=true" >> "$GITHUB_OUTPUT"
          else
            echo "readme_changed=false" >> "$GITHUB_OUTPUT"
          fi

          if grep -qE '^CHANGELOG\.md$' changed_files.txt; then
            echo "changelog_changed=true" >> "$GITHUB_OUTPUT"
          else
            echo "changelog_changed=false" >> "$GITHUB_OUTPUT"
          fi
  
  # --- Phase 4: Content Creation ---
  create-pages:
    name: "Create Missing Wiki Pages"
    runs-on: ubuntu-latest
    needs: [setup-env, check-pages-folders]
    if: needs.check-pages-folders.outputs.has_repo_path == 'false' || needs.check-pages-folders.outputs.missing_count > 0
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download helper scripts
        uses: actions/download-artifact@v4
        with:
          name: fix_paths_script
          path: scripts

      - name: Prepare shared functions
        run: |
          set -euo pipefail
          cat > scripts/wiki_utils.sh <<'EOF'
          #!/usr/bin/env bash
          set -euo pipefail

          source scripts/fix_paths.sh

          normalize_markdown_links() {
            sed -E \
              -e 's#\]\([[:space:]]*/documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md([?#][^)]*)\)#](./documentation/\1\2)#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md\)#](./documentation/\1)#g'
          }

          create_wiki_page() {
            local file="$1"
            local path="$2"
            local title="$3"
            local content="$4"

            local QUERY='mutation ($content: String!, $locale: String!, $path: String!, $title: String!) {
              pages {
                create(
                  path: $path,
                  title: $title,
                  content: $content,
                  editor: "markdown",
                  isPrivate: false,
                  isPublished: true,
                  locale: $locale,
                  tags: [],
                  description: ""
                ) {
                  responseResult { succeeded slug message }
                  page { id title path updatedAt }
                }
              }
            }'

            local PAYLOAD=$(jq -n \
              --arg q "$QUERY" \
              --arg content "$content" \
              --arg loc "$WIKI_LOCALE" \
              --arg path "$path" \
              --arg title "$title" \
              '{query: $q, variables: { content: $content, locale: $loc, path: $path, title: $title }}')

            local RESULT=$(curl -s -X POST "$WIKI_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$PAYLOAD")

            local SUCCEEDED=$(echo "$RESULT" | jq -r '.data.pages.create.responseResult.succeeded // empty')
            if [ "$SUCCEEDED" = "true" ]; then
              echo "‚úÖ Created $title at $path"
            else
              echo "‚ö†Ô∏è Failed to create $title ($file)"
              echo "$RESULT" | jq -r '.data.pages.create.responseResult.message'
              exit 1
            fi
          }
          EOF
          chmod +x scripts/wiki_utils.sh

      - name: Create pages in Wiki.js
        run: |
          set -euo pipefail
          source scripts/wiki_utils.sh

          REPO_SLUG=$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')
          REPO_PATH="${BASE_PATH}/${REPO_SLUG}"
          DOC_PREFIX="${REPO_PATH}/documentation"
          echo "üìÇ Base path: $REPO_PATH"

          # Helper for image paths
          ASSET_PATH_SAFE="${{ needs.setup-env.outputs.ASSET_PATH_SAFE }}"
          export ASSET_PATH_SAFE

          # --- Create README ---
          if [ "${{ needs.check-pages-folders.outputs.has_readme_page }}" = "false" ] && [ -f README.md ]; then
            echo "ü™∂ Creating README..."
            CONTENT=$(tr -d '\r' < README.md | fix_image_paths | normalize_markdown_links)
            create_wiki_page "README.md" "${REPO_PATH}/readme" "README" "$CONTENT"
          fi

          # --- Create CHANGELOG ---
          if [ "${{ needs.check-pages-folders.outputs.has_changelog_page }}" = "false" ] && [ -f CHANGELOG.md ]; then
            echo "üìú Creating CHANGELOG..."
            CONTENT=$(tr -d '\r' < CHANGELOG.md | fix_image_paths | normalize_markdown_links)
            create_wiki_page "CHANGELOG.md" "${REPO_PATH}/changelog" "CHANGELOG" "$CONTENT"
          fi

          # --- Create missing documentation files ---
          MISSING_FILES=(${{ needs.check-pages-folders.outputs.missing_files }})
          if [ "${#MISSING_FILES[@]}" -gt 0 ]; then
            echo "üìò Creating ${#MISSING_FILES[@]} missing documentation files..."
            for FILE in "${MISSING_FILES[@]}"; do
              SRC="documentation/$(basename "$FILE").md"
              [ ! -f "$SRC" ] && echo "‚ö†Ô∏è Missing local file $SRC" && continue
              TITLE=$(basename "$FILE")
              CONTENT=$(tr -d '\r' < "$SRC" | fix_image_paths | normalize_markdown_links)
              create_wiki_page "$SRC" "$FILE" "$TITLE" "$CONTENT"
            done
          else
            echo "‚úÖ No missing documentation files to create."
          fi

  # --- Phase 5: Content Updates ---
  update-pages:
    name: "Update Wiki Pages"
    runs-on: ubuntu-latest
    needs: [setup-env, check-pages-folders, check-altered-files]
    if: needs.check-pages-folders.outputs.has_repo_path == 'true'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download helper scripts
        uses: actions/download-artifact@v4
        with:
          name: fix_paths_script
          path: scripts

      - name: Prepare shared functions
        run: |
          set -euo pipefail
          cat > scripts/wiki_utils.sh <<'EOF'
          #!/usr/bin/env bash
          set -euo pipefail
          source scripts/fix_paths.sh

          # ------------------------------------------------------------
          # Normaliza√ß√£o de links internos em Markdown
          # ------------------------------------------------------------
          normalize_markdown_links() {
            sed -E \
              -e 's#\]\([[:space:]]*/documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md([?#][^)]*)\)#](./documentation/\1\2)#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md\)#](./documentation/\1)#g'
          }

          # ------------------------------------------------------------
          # Obt√©m o ID de uma p√°gina existente na Wiki.js
          # Pesquisa apenas dentro de BASE_PATH (n√£o em toda a wiki)
          # Tolerante a diferen√ßas de mai√∫sculas e acentua√ß√£o
          # ------------------------------------------------------------
          get_page_id() {
            local path="$1"
            local segment
            segment=$(basename "$path")

            local QUERY='query ($locale: String!) {
              pages {
                list(locale: $locale) {
                  id
                  title
                  path
                }
              }
            }'

            local PAYLOAD=$(jq -n \
              --arg q "$QUERY" \
              --arg loc "$WIKI_LOCALE" \
              '{ query: $q, variables: { locale: $loc } }')

            local RESULT=$(curl -sS -X POST "$WIKI_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$PAYLOAD")

            echo "$RESULT" | jq -r \
              --arg base "$BASE_PATH" \
              --arg seg "$segment" '
                .data.pages.list[]
                | select(.path | startswith($base))
                | select(
                    (.path | ascii_downcase | gsub("[^a-z0-9/_-]"; "")) 
                    | endswith(($seg | ascii_downcase | gsub("[^a-z0-9/_-]"; "")))
                  )
                | .id
              ' | head -n 1
          }

          # ------------------------------------------------------------
          # Atualiza o conte√∫do de uma p√°gina existente
          # ------------------------------------------------------------
          update_wiki_page() {
            local id="$1"
            local content="$2"
            local title="$3"

            local QUERY='mutation ($id: Int!, $content: String!, $locale: String!) {
              pages {
                update(id: $id, content: $content, editor: "markdown",
                       isPrivate: false, isPublished: true, locale: $locale, tags: []) {
                  responseResult { succeeded message }
                }
              }
            }'

            local PAYLOAD=$(jq -n \
              --arg q "$QUERY" \
              --argjson id "$id" \
              --arg content "$content" \
              --arg loc "$WIKI_LOCALE" \
              '{query: $q, variables: { id: $id, content: $content, locale: $loc }}')

            local RESULT=$(curl -s -X POST "$WIKI_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$PAYLOAD")

            local SUCCEEDED=$(echo "$RESULT" | jq -r '.data.pages.update.responseResult.succeeded // empty')
            if [ "$SUCCEEDED" = "true" ]; then
              echo "‚úÖ Updated $title successfully."
            else
              echo "‚ö†Ô∏è Failed to update $title"
              echo "$RESULT" | jq -r '.data.pages.update.responseResult.message'
              exit 1
            fi
          }
          EOF
          chmod +x scripts/wiki_utils.sh

      - name: Update changed pages
        run: |
          set -euo pipefail
          source scripts/wiki_utils.sh

          REPO_SLUG=$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')
          REPO_PATH="${BASE_PATH}/${REPO_SLUG}"
          echo "üìÇ Repository path: $REPO_PATH"

          ASSET_PATH_SAFE="${{ needs.setup-env.outputs.ASSET_PATH_SAFE }}"
          export ASSET_PATH_SAFE

          # Lista de ficheiros em falta (detetados na fase 2)
          MISSING_FILES=(${{ needs.check-pages-folders.outputs.missing_files }})
          echo "üß© Missing files (n√£o atualiz√°veis): ${MISSING_FILES[*]}"

          # ------------------------------------------------------------
          # Atualiza uma p√°gina se foi alterada e j√° existe na Wiki
          # ------------------------------------------------------------
          update_if_changed() {
            local file="$1"
            local title="$2"
            local subpath="$3"
            local changed_flag="$4"

            if [ "$changed_flag" = "true" ] && [ -f "$file" ]; then
              local path="${REPO_PATH}/${subpath}"

              # Ignorar se for um ficheiro ainda n√£o criado
              for MISS in "${MISSING_FILES[@]}"; do
                [[ "$MISS" == *"${subpath}"* ]] && echo "‚ö†Ô∏è Skipping $subpath (still missing in Wiki)" && return
              done

              echo "üîç Checking page at $path"
              PAGE_ID=$(get_page_id "$path")
              if [ -z "$PAGE_ID" ]; then
                echo "‚ö†Ô∏è Page not found in Wiki ($path)."
                return
              fi

              echo "üÜî Page ID: $PAGE_ID"
              CONTENT=$(tr -d '\r' < "$file" | fix_image_paths | normalize_markdown_links)
              update_wiki_page "$PAGE_ID" "$CONTENT" "$title"
            fi
          }

          # --- README ---
          update_if_changed "README.md" "README" "readme" "${{ needs.check-altered-files.outputs.readme_changed }}"

          # --- CHANGELOG ---
          update_if_changed "CHANGELOG.md" "CHANGELOG" "changelog" "${{ needs.check-altered-files.outputs.changelog_changed }}"

          # --- Documentation files ---
          if [ "${{ needs.check-altered-files.outputs.docs_changed }}" = "true" ]; then
            CHANGED_DOCS=(${{ needs.check-altered-files.outputs.doc_files_changed }})

            # Remover ficheiros que ainda n√£o existem na Wiki
            for MISS in "${MISSING_FILES[@]}"; do
              MISS_NAME=$(basename "$MISS")
              CHANGED_DOCS=("${CHANGED_DOCS[@]/documentation\/$MISS_NAME.md}")
            done

            echo "üìÑ Documentation files to update: ${CHANGED_DOCS[*]}"

            for FILE in "${CHANGED_DOCS[@]}"; do
              DOC_TITLE=$(basename "$FILE" .md)
              DOC_PATH="${REPO_PATH}/documentation/${DOC_TITLE}"
              PAGE_ID=$(get_page_id "$DOC_PATH")
              [ -z "$PAGE_ID" ] && echo "‚ö†Ô∏è Skipping: no Wiki page for ${DOC_TITLE}" && continue
              CONTENT=$(tr -d '\r' < "documentation/${DOC_TITLE}.md" | fix_image_paths | normalize_markdown_links)
              update_wiki_page "$PAGE_ID" "$CONTENT" "$DOC_TITLE"
            done
          fi

  # --- Phase 6: Asset Upload ---
  upload-docs-media:
    name: "Upload Documentation Media"
    runs-on: ubuntu-latest
    needs: [setup-env, check-pages-folders]
    if: needs.check-pages-folders.outputs.has_repo_path == 'false'
    env:
      # removed BASE_PATH_SAFE (unused)
      ASSET_PATH_SAFE: ${{ needs.setup-env.outputs.ASSET_PATH_SAFE }}
      PARENT_FOLDER_ID: "1"
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Detect Media Files
        id: detect
        run: |
          MEDIA_FILES=$(find documentation/ -type f ! -name "*.md")
          echo "MEDIA_FILES<<EOF" >> "$GITHUB_ENV"
          echo "$MEDIA_FILES" >> "$GITHUB_ENV"
          echo "EOF" >> "$GITHUB_ENV"
          echo "Ficheiros encontrados:"
          echo "$MEDIA_FILES"

      - name: Get Repository Name
        id: repo
        run: |
          repo_name="${GITHUB_REPOSITORY##*/}"
          echo "REPO_NAME=${repo_name,,}" >> "$GITHUB_ENV"
          echo "Repository name: ${repo_name,,}"

      - name: Criar estrutura de pastas aninhadas no Wiki.js
        id: criar-pastas
        env:
          WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN }}
          APP_NAME: ${{ inputs.app_name || github.event.repository.name }}
        run: |
          set -euo pipefail

          # === CONFIGURA√á√ÉO ===
          APP_NAME="${APP_NAME,,}"  # lowercase
          if [ -n "${ASSET_PATH_SAFE:-}" ]; then
            ASSET_FOLDER_PATH="${ASSET_PATH_SAFE}/documentation"
          else
            ASSET_FOLDER_PATH="${BASE_PATH}/${APP_NAME}/documentation"
          fi

          BASE_URL="${WIKI_URL%/}"
          if [[ "$BASE_URL" =~ /graphql$ ]]; then
            GRAPHQL_URL="$BASE_URL"
            BASE_URL="${BASE_URL%/graphql}"
            BASE_URL="${BASE_URL%/}"
            UPLOAD_URL="${BASE_URL}/u"
          else
            GRAPHQL_URL="${BASE_URL}/graphql"
            UPLOAD_URL="${BASE_URL}/u"
          fi

          # Fun√ß√£o: slugificar (imitando comportamento Wiki.js)
          slugify() {
            echo "$1" \
              | tr '[:upper:]' '[:lower:]' \
              | sed -E 's/[[:space:]]+/-/g' \
              | sed -E 's/[^a-z0-9._-]+/-/g' \
              | sed -E 's/-{2,}/-/g' \
              | sed -E 's/^-+|-+$//g'
          }

          # Fun√ß√£o: obter ID da pasta
          get_folder_id() {
            local parent_id="$1"
            local name_raw="$2"
            [[ -z "$parent_id" ]] && parent_id=0

            local name_lc slug
            name_lc="$(echo "$name_raw" | tr '[:upper:]' '[:lower:]')"
            slug="$(slugify "$name_raw")"

            local QUERY JSON RESULT
            QUERY="query { assets { folders(parentFolderId: $parent_id) { id name slug } } }"
            JSON=$(jq -n --arg q "$QUERY" '{ query: $q }')
            RESULT=$(curl -s -X POST "$GRAPHQL_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$JSON")

            echo "$RESULT" | jq -r --arg SLUG "$slug" --arg NAME_LC "$name_lc" '
              .data.assets.folders[]? 
              | select((.slug == $SLUG) or ((.name|ascii_downcase) == $NAME_LC)) 
              | .id
            '
          }

          # Fun√ß√£o: criar pasta
          create_folder() {
            local parent_id="$1"
            local name_raw="$2"
            [[ -z "$parent_id" ]] && parent_id=0
            local slug
            slug="$(slugify "$name_raw")"

            local MUTATION JSON RESULT
            MUTATION="mutation { assets { createFolder(parentFolderId: $parent_id, slug: \"$slug\", name: \"$name_raw\") { responseResult { succeeded message } } } }"
            JSON=$(jq -n --arg q "$MUTATION" '{ query: $q }')
            RESULT=$(curl -s -X POST "$GRAPHQL_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$JSON")

            echo "$RESULT" | jq .
            local SUCCESS
            SUCCESS=$(echo "$RESULT" | jq -r '.data.assets.createFolder.responseResult.succeeded')
            if [ "$SUCCESS" != "true" ]; then
              echo "Erro a criar pasta '$name_raw'"
              exit 1
            fi
          }

          echo "Estrutura alvo: $ASSET_FOLDER_PATH"
          IFS='/' read -ra PARTS <<< "$ASSET_FOLDER_PATH"

          CURRENT_PARENT=""
          for PART in "${PARTS[@]}"; do
            echo "‚Üí Verificar/criar '$PART' sob parent '${CURRENT_PARENT:-0}'"
            FOLDER_ID="$(get_folder_id "$CURRENT_PARENT" "$PART" | head -n1 || true)"

            if [[ -z "$FOLDER_ID" || "$FOLDER_ID" == "null" ]]; then
              echo "‚Ä¶ n√£o existe, a criar '$PART'‚Ä¶"
              create_folder "$CURRENT_PARENT" "$PART"
              FOLDER_ID="$(get_folder_id "$CURRENT_PARENT" "$PART" | head -n1 || true)"
              if [[ -z "$FOLDER_ID" || "$FOLDER_ID" == "null" ]]; then
                echo "Falha a obter ID da pasta '$PART' ap√≥s cria√ß√£o."
                exit 1
              fi
              echo "‚úì criada: id=$FOLDER_ID"
            else
              echo "‚úì j√° existe: id=$FOLDER_ID"
            fi

            CURRENT_PARENT="$FOLDER_ID"
          done

          echo "FOLDER_ID final = $FOLDER_ID"
          echo "FOLDER_ID=$FOLDER_ID" >> "$GITHUB_ENV"
          echo "UPLOAD_URL=$UPLOAD_URL" >> "$GITHUB_ENV"
          echo "GRAPHQL_URL=$GRAPHQL_URL" >> "$GITHUB_ENV"

      - name: Upload Media Files
        env:
          WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN }}
          APP_NAME: ${{ inputs.app_name || github.event.repository.name }}
        run: |
          set -euo pipefail
          GRAPHQL_URL="${GRAPHQL_URL:-$WIKI_URL}"
          UPLOAD_URL="${UPLOAD_URL:-${WIKI_URL%/}/u}"
          if [[ "$UPLOAD_URL" =~ /graphql/u$ ]]; then
            TRIMMED="${UPLOAD_URL%/graphql/u}"
            TRIMMED="${TRIMMED%/}"
            UPLOAD_URL="${TRIMMED}/u"
          fi

          DOC_ROOT_FOLDER_ID="${DOC_ROOT_FOLDER_ID:-${FOLDER_ID:-}}"
          if [ -z "$DOC_ROOT_FOLDER_ID" ]; then
            echo "ERROR: DOC_ROOT_FOLDER_ID not defined. Previous step may have failed."
            exit 1
          fi

          if [ -z "${MEDIA_FILES:-}" ]; then
            echo "No media files to upload."
            exit 0
          fi

          call_graphql() {
            local payload="$1"
            curl -s -X POST "$GRAPHQL_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$payload"
          }

          sanitize_slug() {
            local slug
            slug=$(echo "$1" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9_-]/-/g')
            slug=$(echo "$slug" | sed 's/-\{2,\}/-/g; s/^-//; s/-$//')
            if [ -z "$slug" ]; then
              slug="assets-folder"
            fi
            echo "$slug"
          }

          folders_query='query($parent: Int!) { assets { folders(parentFolderId: $parent) { id name slug } } }'
          create_folder_mutation='mutation($parent: Int!, $slug: String!, $name: String!) { assets { createFolder(parentFolderId: $parent, slug: $slug, name: $name) { responseResult { succeeded message }, folder { id name slug } } } }'

          declare -A FOLDER_CACHE=()

          ensure_child_folder() {
            local parent="$1"
            local name="$2"
            local slug="$3"
            local cache_key="${parent}:${slug}"
            if [[ -n "${FOLDER_CACHE[$cache_key]:-}" ]]; then
              echo "${FOLDER_CACHE[$cache_key]}"
              return 0
            fi

            local payload result folder_id
            payload=$(jq -n \
              --arg q "$folders_query" \
              --argjson parent "$parent" \
              '{ query: $q, variables: { parent: $parent } }')
            result=$(call_graphql "$payload")
            if ! echo "$result" | jq . >/dev/null 2>&1; then
              echo "ERROR: Non-JSON response when querying folders (parent $parent):"
              echo "$result"
              exit 1
            fi
            folder_id=$(echo "$result" | jq -r --arg name "$name" --arg slug "$slug" '
              (.data.assets.folders // [])[]
              | select(
                  (.name // "" | ascii_downcase) == ($name | ascii_downcase)
                  or (.slug // "" | ascii_downcase) == ($slug | ascii_downcase)
                )
              | .id' | head -n 1)
            if [ -n "$folder_id" ] && [ "$folder_id" != "null" ]; then
              FOLDER_CACHE["$cache_key"]="$folder_id"
              echo "$folder_id"
              return 0
            fi

            payload=$(jq -n \
              --arg q "$create_folder_mutation" \
              --argjson parent "$parent" \
              --arg slug "$slug" \
              --arg name "$name" \
              '{ query: $q, variables: { parent: $parent, slug: $slug, name: $name } }')
            result=$(call_graphql "$payload")
            if ! echo "$result" | jq . >/dev/null 2>&1; then
              echo "ERROR: Non-JSON response when creating folder '$name' (parent $parent):"
              echo "$result"
              exit 1
            fi
            local succeeded
            succeeded=$(echo "$result" | jq -r '.data.assets.createFolder.responseResult.succeeded // "false"')
            if [ "$succeeded" != "true" ]; then
              echo "ERROR: Wiki.js reported failure when creating folder '$name' (parent $parent):"
              echo "$result" | jq .
              exit 1
            fi
            folder_id=$(echo "$result" | jq -r '.data.assets.createFolder.folder.id // .data.assets.createFolder.id')
            if [ -z "$folder_id" ] || [ "$folder_id" = "null" ]; then
              echo "ERROR: Folder ID missing after creating '$name' (parent $parent)."
              echo "$result" | jq .
              exit 1
            fi
            FOLDER_CACHE["$cache_key"]="$folder_id"
            echo "$folder_id"
          }

          ensure_path() {
            local parent="$1"
            local path="$2"
            if [ -z "$path" ] || [ "$path" = "." ]; then
              echo "$parent"
              return 0
            fi

            local current="$parent"
            local oldIFS="$IFS"
            IFS='/'
            read -r -a segments <<< "$path"
            IFS="$oldIFS"
            for segment in "${segments[@]}"; do
              [ -z "$segment" ] && continue
              [ "$segment" = "." ] && continue
              if [ "$segment" = ".." ]; then
                echo "ERROR: Unsupported path segment '..' in '$path'."
                exit 1
              fi
              local slug
              slug=$(sanitize_slug "$segment")
              current=$(ensure_child_folder "$current" "$segment" "$slug")
            done
            echo "$current"
          }

          echo "$MEDIA_FILES" | while IFS= read -r filepath || [ -n "$filepath" ]; do
            [ -z "$filepath" ] && continue
            if [ ! -f "$filepath" ]; then
              echo "File '$filepath' not found locally, skipping."
              continue
            fi

            target_folder_id="$DOC_ROOT_FOLDER_ID"
            relative_path="$filepath"
            relative_dir=""
            if [[ "$filepath" == documentation/* ]]; then
              relative_path="${filepath#documentation/}"
              if [[ "$relative_path" == */* ]]; then
                relative_dir="${relative_path%/*}"
                target_folder_id=$(ensure_path "$DOC_ROOT_FOLDER_ID" "$relative_dir")
              fi
            fi

            echo "Uploading $filepath"
            RESPONSE_WITH_STATUS=$(curl -s -w "\n%{http_code}" -X POST "$UPLOAD_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -F "mediaUpload={\"folderId\":$target_folder_id}" \
              -F "mediaUpload=@$filepath")
            HTTP_STATUS=$(echo "$RESPONSE_WITH_STATUS" | tail -n1)
            BODY=$(echo "$RESPONSE_WITH_STATUS" | sed '$d')
            if [[ "$HTTP_STATUS" =~ ^2 ]]; then
              if echo "$BODY" | jq . >/dev/null 2>&1; then
                echo "$BODY" | jq .
              else
                echo "Upload succeeded (HTTP $HTTP_STATUS) for file: $filepath"
                echo "$BODY"
              fi
            else
              echo "Upload failed (HTTP $HTTP_STATUS) for file: $filepath"
              if echo "$BODY" | jq . >/dev/null 2>&1; then
                echo "$BODY" | jq .
              else
                echo "$BODY"
              fi
              exit 1
            fi
          done
