name: WikiDocumentation

# Example usage:
#   jobs:
#     run-docs-pipeline:
#       uses: org/repo/.github/workflows/WikiDocumentation.yaml@main
#       with:
#         app_name: "wiki-upload-test "

on:
  workflow_call:
    inputs:
      app_name:
        description: "Application name to be used when syncing documentation."
        required: true
        type: string
      base_path:
        description: "Base folder path in the Wiki assets (e.g. 'Pedro/Aplica√ß√µes')."
        required: true
        type: string
    secrets:
      WIKI_API_TOKEN:
        description: "API token for Wiki.js GraphQL access"
        required: true

permissions:
  contents: read
  actions: write

env:
  WIKI_URL: "http://wiki.franquinho.info/graphql"
  #WIKI_URL: "http://cyberwiki.internal.ctt.pt/graphql"
  #WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN_WIKILAB2  }}
  WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN }}
  WIKI_LOCALE: "en"
  BASE_PATH: ${{ inputs.base_path }}
  REPO_NAME: ${{ inputs.app_name != '' && inputs.app_name || github.event.repository.name }}

jobs:
  # --- Phase 1: Setup ---
  setup-env:
    name: "Setup Environment"
    runs-on: ubuntu-latest
    outputs:
      # removed BASE_PATH_SAFE (unused)
      ASSET_PATH_SAFE: ${{ steps.export.outputs.ASSET_PATH_SAFE }}
    steps:
      - name: Compute ASSET_PATH_SAFE
        id: export
        run: |
          set -euo pipefail
          # removed BASE_PATH_SAFE (unused)
          raw_base="${BASE_PATH:-}"
          raw_app="${REPO_NAME:-}"

          sanitize_segment() {
            local segment="$1"
            segment=$(echo "$segment" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null)
            segment=$(echo "$segment" | tr '[:upper:]' '[:lower:]')
            segment=$(echo "$segment" | sed 's/[^a-z0-9._-]/-/g')
            segment=$(echo "$segment" | sed 's/-\{2,\}/-/g')
            segment=$(echo "$segment" | sed 's/^-//; s/-$//')
            echo "$segment"
          }

          asset_path="${raw_base}/${raw_app}"
          asset_safe=""
          oldIFS="$IFS"
          IFS='/'
          read -r -a segments <<< "$asset_path"
          IFS="$oldIFS"
          for segment in "${segments[@]}"; do
            [ -z "$segment" ] && continue
            clean_segment=$(sanitize_segment "$segment")
            [ -z "$clean_segment" ] && continue
            asset_safe="${asset_safe}/${clean_segment}"
          done
          asset_safe="${asset_safe#/}"
          asset_safe="${asset_safe%/}"
          echo "ASSET_PATH_SAFE=$asset_safe"
          echo "ASSET_PATH_SAFE=$asset_safe" >> "$GITHUB_OUTPUT"

      - name: Save shared Bash functions
        run: |
          mkdir -p scripts
          cat > scripts/fix_paths.sh <<'EOF'
          #!/usr/bin/env bash
          set -euo pipefail

          fix_image_paths() {
            local prefix="/${ASSET_PATH_SAFE}/documentation/"

            sed -E "
              # 1Ô∏è‚É£ Corrige links tipo ./documentation/...
              s#\]\((<?)(\./)?documentation/#](\1${prefix}#g;

              # 2Ô∏è‚É£ Corrige refer√™ncia [id]: ./documentation/...
              s#^(\[[^\]]+\]:[[:space:]]*)(<?)(\./)?documentation/#\1\2${prefix}#g;

              # 3Ô∏è‚É£ Corrige imagens simples sem 'documentation/'
              # Ex: ![alt text](imagem.png)
              s#(!\[[^]]*\]\()[[:space:]]*([A-Za-z0-9._-]+\.(png|jpg|jpeg|gif|svg|webp|drawio|pdf))[[:space:]]*\)#\1${prefix}\2)#g;
            "
          }

          EOF
      - name: Upload helper script for later jobs
        uses: actions/upload-artifact@v4
        with:
          name: fix_paths_script
          path: scripts/fix_paths.sh

  # --- Phase 2: Wiki Discovery ---
  check-pages-folders:
    name: "Check if Pages and Folders are created"
    runs-on: ubuntu-latest
    needs: [setup-env]
    outputs:
      has_repo_path: ${{ steps.check.outputs.has_repo_path }}
      has_documentation_folder: ${{ steps.check.outputs.has_documentation_folder }}
      has_readme_page: ${{ steps.check.outputs.has_readme_page }}
      has_changelog_page: ${{ steps.check.outputs.has_changelog_page }}
      repo_path: ${{ steps.check.outputs.repo_path }}
      missing_files: ${{ steps.check.outputs.missing_files }}
      docs_files: ${{ steps.check.outputs.docs_files }}
      missing_count: ${{ steps.check.outputs.missing_count }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Query Wiki.js and verify structure
        id: check
        run: |
          set -euo pipefail

          # --- Paths ---
          REPO_SLUG="$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null \
            | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')"

          REPO_PATH="$(echo "${BASE_PATH}/${REPO_SLUG}" | sed 's#//*#/#g; s#/$##')"
          DOC_PREFIX="${REPO_PATH}/documentation/"
          README_PATH="${REPO_PATH}/readme"
          CHANGELOG_PATH="${REPO_PATH}/changelog"

          echo "üîç Checking Wiki structure for ${REPO_PATH}"

          # --- Query restricted to tagged pages ---
          QUERY=$(cat <<'GRAPHQL'
          query ($locale: String!) {
            pages {
              list(locale: $locale, tags: ["gh-sync"]) {
                id
                path
                title
                tags
              }
            }
          }
          GRAPHQL
          )

          JSON_PAYLOAD=$(jq -n --arg q "$QUERY" --arg loc "$WIKI_LOCALE" \
            '{query: $q, variables: { locale: $loc }}')

          RESULT=$(curl -sS -X POST "$WIKI_URL" \
            -H "Authorization: Bearer $WIKI_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "$JSON_PAYLOAD")

          # --- Verifica√ß√£o de erros ---
          if jq -e '.errors' >/dev/null 2>&1 <<<"$RESULT"; then
            echo "‚ùå GraphQL error:"; echo "$RESULT" | jq '.errors'; exit 1
          fi

          # --- Flags principais ---
          HAS_REPO_PATH=$(echo "$RESULT" | jq --arg path "$REPO_PATH" \
            '[.data.pages.list[] | select(.path | startswith($path))] | length > 0')
          HAS_DOCUMENTATION_FOLDER=$(echo "$RESULT" | jq --arg path "$DOC_PREFIX" \
            '[.data.pages.list[] | select(.path | startswith($path))] | length > 0')
          HAS_README_PAGE=$(echo "$RESULT" | jq --arg path "$README_PATH" \
            '[.data.pages.list[] | select(.path == $path)] | length > 0')
          HAS_CHANGELOG_PAGE=$(echo "$RESULT" | jq --arg path "$CHANGELOG_PATH" \
            '[.data.pages.list[] | select(.path == $path)] | length > 0')

          # --- Ficheiros locais ---
          DOC_FILES=($(find documentation -type f -name "*.md" -exec basename {} .md \;))
          MISSING_FILES=()

          for FILE in "${DOC_FILES[@]}"; do
            FILE_PATH="${DOC_PREFIX}${FILE}"
            EXISTS=$(echo "$RESULT" | jq --arg path "$FILE_PATH" \
              '[.data.pages.list[] | select(.path == $path)] | length > 0')
            [ "$EXISTS" != "true" ] && MISSING_FILES+=("$FILE_PATH")
          done

          # --- Outputs ---
          echo "has_repo_path=$HAS_REPO_PATH" >> "$GITHUB_OUTPUT"
          echo "has_documentation_folder=$HAS_DOCUMENTATION_FOLDER" >> "$GITHUB_OUTPUT"
          echo "has_readme_page=$HAS_README_PAGE" >> "$GITHUB_OUTPUT"
          echo "has_changelog_page=$HAS_CHANGELOG_PAGE" >> "$GITHUB_OUTPUT"
          echo "repo_path=$REPO_PATH" >> "$GITHUB_OUTPUT"
          echo "missing_files=${MISSING_FILES[*]}" >> "$GITHUB_OUTPUT"
          echo "missing_count=${#MISSING_FILES[@]}" >> "$GITHUB_OUTPUT"
          echo "docs_files=${DOC_FILES[*]}" >> "$GITHUB_OUTPUT"

          echo "‚úÖ Wiki discovery complete."

  # --- Phase 3: Change Detection ---
  check-altered-files:
    name: "Check Changed Files"
    runs-on: ubuntu-latest
    needs: [setup-env]
    outputs:
      DOCS_CHANGED: ${{ steps.check.outputs.DOCS_CHANGED }}
      README_CHANGED: ${{ steps.check.outputs.README_CHANGED }}
      CHANGELOG_CHANGED: ${{ steps.check.outputs.CHANGELOG_CHANGED }}
      DOC_FILES_CHANGED: ${{ steps.check.outputs.DOC_FILES_CHANGED }}
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0   # precisamos do hist√≥rico completo para o git diff funcionar corretamente

      - name: Get changed files
        id: changed-files
        run: |
          set -euo pipefail
          BEFORE="${{ github.event.before }}"
          ZERO="0000000000000000000000000000000000000000"

          echo "üîç Determining changed files..."
          if [ -z "$BEFORE" ] || [ "$BEFORE" = "$ZERO" ]; then
            # Se for o primeiro commit ou run manual
            if git rev-parse HEAD^ >/dev/null 2>&1; then
              git diff --name-only --diff-filter=AM HEAD^ HEAD > changed_files.txt
            else
              git show --name-only --pretty="" --diff-filter=AM HEAD > changed_files.txt
            fi
          else
            # S√≥ compara o intervalo real
            git diff --name-only --diff-filter=AM "$BEFORE" ${{ github.sha }} > changed_files.txt
          fi

          echo "Changed files:"
          cat changed_files.txt || echo "(none)"
          echo "----"
          # Remove duplicados e linhas vazias
          awk 'NF && !a[$0]++' changed_files.txt > tmp && mv tmp changed_files.txt
          cat changed_files.txt

      - name: Check if documentation files changed
        id: check
        run: |
          set -euo pipefail

          DOC_FILES_CHANGED=$(grep -E '^documentation/.+\.md$' changed_files.txt || true)
          DOC_FILES_CHANGED=$(echo "$DOC_FILES_CHANGED" | tr -d '\r' | sort -u | tr '\n' ' ' | xargs)

          if [ -n "$DOC_FILES_CHANGED" ]; then
            echo "üìÑ Documentation files changed: $DOC_FILES_CHANGED"
            echo "DOCS_CHANGED=true" >> "$GITHUB_OUTPUT"
            echo "DOC_FILES_CHANGED=$DOC_FILES_CHANGED" >> "$GITHUB_OUTPUT"
          else
            echo "üìÑ No documentation files changed."
            echo "DOCS_CHANGED=false" >> "$GITHUB_OUTPUT"
            echo "DOC_FILES_CHANGED=" >> "$GITHUB_OUTPUT"
          fi

          if grep -qE '^README\.md$' changed_files.txt; then
            echo "üìò README.md changed."
            echo "README_CHANGED=true" >> "$GITHUB_OUTPUT"
          else
            echo "README_CHANGED=false" >> "$GITHUB_OUTPUT"
          fi

          if grep -qE '^CHANGELOG\.md$' changed_files.txt; then
            echo "üìò CHANGELOG.md changed."
            echo "CHANGELOG_CHANGED=true" >> "$GITHUB_OUTPUT"
          else
            echo "CHANGELOG_CHANGED=false" >> "$GITHUB_OUTPUT"
          fi

  
  # --- Phase 4: Content Creation ---
  create-pages:
    name: "Create Missing Wiki Pages"
    runs-on: ubuntu-latest
    needs: [setup-env, check-pages-folders]
    if: needs.check-pages-folders.outputs.has_repo_path == 'false' || needs.check-pages-folders.outputs.missing_count > 0

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download helper scripts
        uses: actions/download-artifact@v4
        with:
          name: fix_paths_script
          path: scripts

      - name: Prepare shared functions
        run: |
          set -euo pipefail
          mkdir -p scripts
          cat <<'EOF' > scripts/wiki_utils.sh
          #!/usr/bin/env bash
          set -euo pipefail
          source scripts/fix_paths.sh

          normalize_markdown_links() {
            sed -E \
              -e 's#\]\([[:space:]]*/documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md([?#][^)]*)\)#](./documentation/\1\2)#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md\)#](./documentation/\1)#g'
          }

          # Create a new page in Wiki.js with tag "gh-sync"
          create_wiki_page() {
            local src="$1"
            local path="$2"
            local title="$3"
            local content="$4"

            echo "üÜï Creating page: $path"

            local QUERY='mutation ($path: String!, $title: String!, $content: String!, $locale: String!) {
              pages {
                create(
                  path: $path,
                  title: $title,
                  content: $content,
                  editor: "markdown",
                  isPrivate: false,
                  isPublished: true,
                  locale: $locale,
                  tags: ["gh-sync"]
                ) {
                  responseResult { succeeded message }
                  page { id path title tags }
                }
              }
            }'

            local PAYLOAD
            PAYLOAD=$(jq -n \
              --arg q "$QUERY" \
              --arg path "$path" \
              --arg title "$title" \
              --arg content "$content" \
              --arg loc "$WIKI_LOCALE" \
              '{ query: $q, variables: { path: $path, title: $title, content: $content, locale: $loc } }')

            local RESULT
            RESULT=$(curl -sS -X POST "$WIKI_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$PAYLOAD")

            local OK
            OK=$(echo "$RESULT" | jq -r '.data.pages.create.responseResult.succeeded // empty')

            if [ "$OK" = "true" ]; then
              local NEW_ID
              NEW_ID=$(echo "$RESULT" | jq -r '.data.pages.create.page.id')
              echo "‚úÖ Created page \"$title\" (ID: $NEW_ID)"
            else
              echo "‚ö†Ô∏è Failed to create page \"$title\""
              echo "$RESULT" | jq '.errors // .data.pages.create.responseResult'
              exit 1
            fi
          }
          EOF
          chmod +x scripts/wiki_utils.sh

      - name: Create pages in Wiki.js
        run: |
          set -euo pipefail
          source scripts/wiki_utils.sh

          REPO_SLUG=$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')
          REPO_PATH="${BASE_PATH}/${REPO_SLUG}"
          DOC_PREFIX="${REPO_PATH}/documentation"
          echo "üìÇ Base path: $REPO_PATH"

          ASSET_PATH_SAFE="${{ needs.setup-env.outputs.ASSET_PATH_SAFE }}"
          export ASSET_PATH_SAFE

          # README
          if [ "${{ needs.check-pages-folders.outputs.has_readme_page }}" = "false" ] && [ -f README.md ]; then
            echo "ü™∂ Creating README..."
            CONTENT=$(tr -d '\r' < README.md | fix_image_paths | normalize_markdown_links)
            create_wiki_page "README.md" "${REPO_PATH}/readme" "README" "$CONTENT"
          fi

          # CHANGELOG
          if [ "${{ needs.check-pages-folders.outputs.has_changelog_page }}" = "false" ] && [ -f CHANGELOG.md ]; then
            echo "üìú Creating CHANGELOG..."
            CONTENT=$(tr -d '\r' < CHANGELOG.md | fix_image_paths | normalize_markdown_links)
            create_wiki_page "CHANGELOG.md" "${REPO_PATH}/changelog" "CHANGELOG" "$CONTENT"
          fi

          # Documentation files
          MISSING_FILES=(${{ needs.check-pages-folders.outputs.missing_files }})
          if [ "${#MISSING_FILES[@]}" -gt 0 ]; then
            echo "üìò Creating ${#MISSING_FILES[@]} missing documentation files..."
            for FILE in "${MISSING_FILES[@]}"; do
              SRC="documentation/$(basename "$FILE").md"
              [ ! -f "$SRC" ] && echo "‚ö†Ô∏è Missing local file $SRC" && continue
              TITLE=$(basename "$FILE")
              CONTENT=$(tr -d '\r' < "$SRC" | fix_image_paths | normalize_markdown_links)
              create_wiki_page "$SRC" "$FILE" "$TITLE" "$CONTENT"
            done
          else
            echo "‚úÖ No missing documentation files to create."
          fi

  # --- Phase 5: Content Updates ---
  update-pages:
    name: "Update Wiki Pages"
    runs-on: ubuntu-latest
    needs: [setup-env, check-pages-folders, check-altered-files]
    if: needs.check-pages-folders.outputs.has_repo_path == 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download helper scripts
        uses: actions/download-artifact@v4
        with:
          name: fix_paths_script
          path: scripts

      - name: Prepare shared functions
        run: |
          set -euo pipefail
          mkdir -p scripts
          cat <<'EOF' > scripts/wiki_utils.sh
          #!/usr/bin/env bash
          set -euo pipefail
          source scripts/fix_paths.sh

          normalize_markdown_links() {
            sed -E \
              -e 's#\]\([[:space:]]*/documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*documentation/#](./documentation/#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md([?#][^)]*)\)#](./documentation/\1\2)#g' \
              -e 's#\]\([[:space:]]*\./documentation/([^)]*)\.md\)#](./documentation/\1)#g'
          }

          # Get page ID scoped to current repo
          get_page_id() {
            local rel_path="$1"
            local target="${rel_path#/}"

            local repo_slug
            repo_slug=$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null \
                        | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')

            local repo_prefix="${BASE_PATH}/${repo_slug}"
            repo_prefix="${repo_prefix//\/\//\/}"

            local QUERY='query ($locale: String!) {
              pages {
                list(locale: $locale, tags: ["gh-sync"]) { id title path }
              }
            }'

            local PAYLOAD
            PAYLOAD=$(jq -n --arg q "$QUERY" --arg loc "$WIKI_LOCALE" '{query:$q,variables:{locale:$loc}}')

            local RESULT
            RESULT=$(curl -sS -X POST "$WIKI_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$PAYLOAD")

            echo "$RESULT" | jq -r \
              --arg prefix "$repo_prefix" --arg target "$target" '
                .data.pages.list[]?
                | select(.path | startswith($prefix))
                | select(
                    (.path | ascii_downcase | gsub("√ß";"c") | gsub("√á";"c")) ==
                    ($target | ascii_downcase | gsub("√ß";"c") | gsub("√á";"c"))
                  )
                | .id
              ' | head -n 1
          }

          # Update keeping tag "gh-sync"
          update_wiki_page() {
            local id="$1"
            local content_file="$2"
            local title="$3"

            local CONTENT
            CONTENT=$(tr -d '\r' < "$content_file")

            local QUERY='mutation ($id: Int!, $content: String!, $locale: String!) {
              pages {
                update(
                  id: $id,
                  content: $content,
                  editor: "markdown",
                  isPrivate: false,
                  isPublished: true,
                  locale: $locale,
                  tags: ["gh-sync"]
                ) {
                  responseResult { succeeded message }
                  page { id path updatedAt }
                }
              }
            }'

            local JSON_PAYLOAD
            JSON_PAYLOAD=$(jq -n \
              --arg q "$QUERY" \
              --arg id "$id" \
              --arg content "$CONTENT" \
              --arg loc "$WIKI_LOCALE" \
              '{query: $q, variables: {id: ($id | tonumber), content: $content, locale: $loc}}')

            local RESULT
            RESULT=$(curl -sS -X POST "$WIKI_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$JSON_PAYLOAD")

            local OK
            OK=$(echo "$RESULT" | jq -r '.data.pages.update.responseResult.succeeded // empty')

            if [ "$OK" = "true" ]; then
              echo "‚úÖ $title updated (ID $id)"
            else
              echo "‚ö†Ô∏è  Failed to update $title (ID $id)"
              echo "$RESULT" | jq '.errors // .data.pages.update.responseResult'
              exit 1
            fi
          }
          EOF
          chmod +x scripts/wiki_utils.sh

      - name: Update changed pages
        run: |
          set -euo pipefail
          export ASSET_PATH_SAFE="${{ needs.setup-env.outputs.ASSET_PATH_SAFE }}" || true
          : "${ASSET_PATH_SAFE:=default}"
          source scripts/wiki_utils.sh

          REPO_SLUG=$(echo "$REPO_NAME" | iconv -f UTF-8 -t ASCII//TRANSLIT 2>/dev/null | sed -E 's/[[:space:]]+/-/g; s/[^A-Za-z0-9._-]+//g')
          REPO_PATH="${BASE_PATH}/${REPO_SLUG}"

          RAW_MISSING="${{ needs.check-pages-folders.outputs.missing_files }}"
          IFS=' ' read -r -a MISSING_FILES <<< "$(echo "$RAW_MISSING" | tr -s ' ')"

          RAW_CHANGED="${{ needs.check-altered-files.outputs.DOC_FILES_CHANGED }}"
          IFS=' ' read -r -a CHANGED_DOCS <<< "$(echo "$RAW_CHANGED" | tr -s ' ')"

          DOCS_CHANGED="${{ needs.check-altered-files.outputs.DOCS_CHANGED }}"
          README_CHANGED="${{ needs.check-altered-files.outputs.README_CHANGED }}"
          CHANGELOG_CHANGED="${{ needs.check-altered-files.outputs.CHANGELOG_CHANGED }}"

          update_if_changed() {
            local file="$1"
            local title="$2"
            local subpath="$3"
            local changed="$4"

            if [ "$changed" = "true" ] && [ -f "$file" ]; then
              local path="${REPO_PATH}/${subpath}"

              for MISS in "${MISSING_FILES[@]}"; do
                [[ "$MISS" == *"${subpath}"* ]] && return
              done

              PAGE_ID=$(get_page_id "$path")
              [ -z "$PAGE_ID" ] && return

              local CONTENT
              CONTENT=$(tr -d '\r' < "$file" | fix_image_paths | normalize_markdown_links)
              SHORT_SHA="${GITHUB_SHA:-unknown}"
              CONTENT=$(printf '%s\n\n<!-- synced:%s -->\n' "$CONTENT" "${SHORT_SHA:0:7}")
              local CONTENT_FILE
              CONTENT_FILE="$(mktemp)"
              printf '%s' "$CONTENT" > "$CONTENT_FILE"
              update_wiki_page "$PAGE_ID" "$CONTENT_FILE" "$title"
            fi
          }

          update_if_changed "README.md" "README" "readme" "$README_CHANGED"
          update_if_changed "CHANGELOG.md" "CHANGELOG" "changelog" "$CHANGELOG_CHANGED"

          if [ "$DOCS_CHANGED" = "true" ]; then
            for FILE in "${CHANGED_DOCS[@]}"; do
              [ -z "$FILE" ] && continue
              DOC_TITLE=$(basename "$FILE" .md)
              DOC_PATH="${REPO_PATH}/documentation/${DOC_TITLE}"
              PAGE_ID=$(get_page_id "$DOC_PATH")
              [ -z "$PAGE_ID" ] && continue
              local CONTENT
              CONTENT=$(tr -d '\r' < "documentation/${DOC_TITLE}.md" | fix_image_paths | normalize_markdown_links)
              SHORT_SHA="${GITHUB_SHA:-unknown}"
              CONTENT=$(printf '%s\n\n<!-- synced:%s -->\n' "$CONTENT" "${SHORT_SHA:0:7}")
              local CONTENT_FILE
              CONTENT_FILE="$(mktemp)"
              printf '%s' "$CONTENT" > "$CONTENT_FILE"
              update_wiki_page "$PAGE_ID" "$CONTENT_FILE" "$DOC_TITLE"
            done
          else
            echo "‚ÑπÔ∏è No documentation files changed."
          fi


  # --- Phase 6: Asset Upload ---
  upload-docs-media:
    name: "Upload Documentation Media"
    runs-on: ubuntu-latest
    needs: [setup-env, check-pages-folders]
    if: needs.check-pages-folders.outputs.has_repo_path == 'false'
    env:
      # removed BASE_PATH_SAFE (unused)
      ASSET_PATH_SAFE: ${{ needs.setup-env.outputs.ASSET_PATH_SAFE }}
      PARENT_FOLDER_ID: "1"
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Detect Media Files
        id: detect
        run: |
          MEDIA_FILES=$(find documentation/ -type f ! -name "*.md")
          echo "MEDIA_FILES<<EOF" >> "$GITHUB_ENV"
          echo "$MEDIA_FILES" >> "$GITHUB_ENV"
          echo "EOF" >> "$GITHUB_ENV"
          echo "Ficheiros encontrados:"
          echo "$MEDIA_FILES"

      - name: Get Repository Name
        id: repo
        run: |
          repo_name="${GITHUB_REPOSITORY##*/}"
          echo "REPO_NAME=${repo_name,,}" >> "$GITHUB_ENV"
          echo "Repository name: ${repo_name,,}"

      - name: Criar estrutura de pastas aninhadas no Wiki.js
        id: criar-pastas
        env:
          WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN }}
          APP_NAME: ${{ inputs.app_name || github.event.repository.name }}
        run: |
          set -euo pipefail

          # === CONFIGURA√á√ÉO ===
          APP_NAME="${APP_NAME,,}"  # lowercase
          if [ -n "${ASSET_PATH_SAFE:-}" ]; then
            ASSET_FOLDER_PATH="${ASSET_PATH_SAFE}/documentation"
          else
            ASSET_FOLDER_PATH="${BASE_PATH}/${APP_NAME}/documentation"
          fi

          BASE_URL="${WIKI_URL%/}"
          if [[ "$BASE_URL" =~ /graphql$ ]]; then
            GRAPHQL_URL="$BASE_URL"
            BASE_URL="${BASE_URL%/graphql}"
            BASE_URL="${BASE_URL%/}"
            UPLOAD_URL="${BASE_URL}/u"
          else
            GRAPHQL_URL="${BASE_URL}/graphql"
            UPLOAD_URL="${BASE_URL}/u"
          fi

          # Fun√ß√£o: slugificar (imitando comportamento Wiki.js)
          slugify() {
            echo "$1" \
              | tr '[:upper:]' '[:lower:]' \
              | sed -E 's/[[:space:]]+/-/g' \
              | sed -E 's/[^a-z0-9._-]+/-/g' \
              | sed -E 's/-{2,}/-/g' \
              | sed -E 's/^-+|-+$//g'
          }

          # Fun√ß√£o: obter ID da pasta
          get_folder_id() {
            local parent_id="$1"
            local name_raw="$2"
            [[ -z "$parent_id" ]] && parent_id=0

            local name_lc slug
            name_lc="$(echo "$name_raw" | tr '[:upper:]' '[:lower:]')"
            slug="$(slugify "$name_raw")"

            local QUERY JSON RESULT
            QUERY="query { assets { folders(parentFolderId: $parent_id) { id name slug } } }"
            JSON=$(jq -n --arg q "$QUERY" '{ query: $q }')
            RESULT=$(curl -s -X POST "$GRAPHQL_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$JSON")

            echo "$RESULT" | jq -r --arg SLUG "$slug" --arg NAME_LC "$name_lc" '
              .data.assets.folders[]? 
              | select((.slug == $SLUG) or ((.name|ascii_downcase) == $NAME_LC)) 
              | .id
            '
          }

          # Fun√ß√£o: criar pasta
          create_folder() {
            local parent_id="$1"
            local name_raw="$2"
            [[ -z "$parent_id" ]] && parent_id=0
            local slug
            slug="$(slugify "$name_raw")"

            local MUTATION JSON RESULT
            MUTATION="mutation { assets { createFolder(parentFolderId: $parent_id, slug: \"$slug\", name: \"$name_raw\") { responseResult { succeeded message } } } }"
            JSON=$(jq -n --arg q "$MUTATION" '{ query: $q }')
            RESULT=$(curl -s -X POST "$GRAPHQL_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$JSON")

            echo "$RESULT" | jq .
            local SUCCESS
            SUCCESS=$(echo "$RESULT" | jq -r '.data.assets.createFolder.responseResult.succeeded')
            if [ "$SUCCESS" != "true" ]; then
              echo "Erro a criar pasta '$name_raw'"
              exit 1
            fi
          }

          echo "Estrutura alvo: $ASSET_FOLDER_PATH"
          IFS='/' read -ra PARTS <<< "$ASSET_FOLDER_PATH"

          CURRENT_PARENT=""
          for PART in "${PARTS[@]}"; do
            echo "‚Üí Verificar/criar '$PART' sob parent '${CURRENT_PARENT:-0}'"
            FOLDER_ID="$(get_folder_id "$CURRENT_PARENT" "$PART" | head -n1 || true)"

            if [[ -z "$FOLDER_ID" || "$FOLDER_ID" == "null" ]]; then
              echo "‚Ä¶ n√£o existe, a criar '$PART'‚Ä¶"
              create_folder "$CURRENT_PARENT" "$PART"
              FOLDER_ID="$(get_folder_id "$CURRENT_PARENT" "$PART" | head -n1 || true)"
              if [[ -z "$FOLDER_ID" || "$FOLDER_ID" == "null" ]]; then
                echo "Falha a obter ID da pasta '$PART' ap√≥s cria√ß√£o."
                exit 1
              fi
              echo "‚úì criada: id=$FOLDER_ID"
            else
              echo "‚úì j√° existe: id=$FOLDER_ID"
            fi

            CURRENT_PARENT="$FOLDER_ID"
          done

          echo "FOLDER_ID final = $FOLDER_ID"
          echo "FOLDER_ID=$FOLDER_ID" >> "$GITHUB_ENV"
          echo "UPLOAD_URL=$UPLOAD_URL" >> "$GITHUB_ENV"
          echo "GRAPHQL_URL=$GRAPHQL_URL" >> "$GITHUB_ENV"

      - name: Upload Media Files
        env:
          WIKI_API_TOKEN: ${{ secrets.WIKI_API_TOKEN }}
          APP_NAME: ${{ inputs.app_name || github.event.repository.name }}
        run: |
          set -euo pipefail
          GRAPHQL_URL="${GRAPHQL_URL:-$WIKI_URL}"
          UPLOAD_URL="${UPLOAD_URL:-${WIKI_URL%/}/u}"
          if [[ "$UPLOAD_URL" =~ /graphql/u$ ]]; then
            TRIMMED="${UPLOAD_URL%/graphql/u}"
            TRIMMED="${TRIMMED%/}"
            UPLOAD_URL="${TRIMMED}/u"
          fi

          DOC_ROOT_FOLDER_ID="${DOC_ROOT_FOLDER_ID:-${FOLDER_ID:-}}"
          if [ -z "$DOC_ROOT_FOLDER_ID" ]; then
            echo "ERROR: DOC_ROOT_FOLDER_ID not defined. Previous step may have failed."
            exit 1
          fi

          if [ -z "${MEDIA_FILES:-}" ]; then
            echo "No media files to upload."
            exit 0
          fi

          call_graphql() {
            local payload="$1"
            curl -s -X POST "$GRAPHQL_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -H "Content-Type: application/json" \
              --data "$payload"
          }

          sanitize_slug() {
            local slug
            slug=$(echo "$1" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9_-]/-/g')
            slug=$(echo "$slug" | sed 's/-\{2,\}/-/g; s/^-//; s/-$//')
            if [ -z "$slug" ]; then
              slug="assets-folder"
            fi
            echo "$slug"
          }

          folders_query='query($parent: Int!) { assets { folders(parentFolderId: $parent) { id name slug } } }'
          create_folder_mutation='mutation($parent: Int!, $slug: String!, $name: String!) { assets { createFolder(parentFolderId: $parent, slug: $slug, name: $name) { responseResult { succeeded message }, folder { id name slug } } } }'

          declare -A FOLDER_CACHE=()

          ensure_child_folder() {
            local parent="$1"
            local name="$2"
            local slug="$3"
            local cache_key="${parent}:${slug}"
            if [[ -n "${FOLDER_CACHE[$cache_key]:-}" ]]; then
              echo "${FOLDER_CACHE[$cache_key]}"
              return 0
            fi

            local payload result folder_id
            payload=$(jq -n \
              --arg q "$folders_query" \
              --argjson parent "$parent" \
              '{ query: $q, variables: { parent: $parent } }')
            result=$(call_graphql "$payload")
            if ! echo "$result" | jq . >/dev/null 2>&1; then
              echo "ERROR: Non-JSON response when querying folders (parent $parent):"
              echo "$result"
              exit 1
            fi
            folder_id=$(echo "$result" | jq -r --arg name "$name" --arg slug "$slug" '
              (.data.assets.folders // [])[]
              | select(
                  (.name // "" | ascii_downcase) == ($name | ascii_downcase)
                  or (.slug // "" | ascii_downcase) == ($slug | ascii_downcase)
                )
              | .id' | head -n 1)
            if [ -n "$folder_id" ] && [ "$folder_id" != "null" ]; then
              FOLDER_CACHE["$cache_key"]="$folder_id"
              echo "$folder_id"
              return 0
            fi

            payload=$(jq -n \
              --arg q "$create_folder_mutation" \
              --argjson parent "$parent" \
              --arg slug "$slug" \
              --arg name "$name" \
              '{ query: $q, variables: { parent: $parent, slug: $slug, name: $name } }')
            result=$(call_graphql "$payload")
            if ! echo "$result" | jq . >/dev/null 2>&1; then
              echo "ERROR: Non-JSON response when creating folder '$name' (parent $parent):"
              echo "$result"
              exit 1
            fi
            local succeeded
            succeeded=$(echo "$result" | jq -r '.data.assets.createFolder.responseResult.succeeded // "false"')
            if [ "$succeeded" != "true" ]; then
              echo "ERROR: Wiki.js reported failure when creating folder '$name' (parent $parent):"
              echo "$result" | jq .
              exit 1
            fi
            folder_id=$(echo "$result" | jq -r '.data.assets.createFolder.folder.id // .data.assets.createFolder.id')
            if [ -z "$folder_id" ] || [ "$folder_id" = "null" ]; then
              echo "ERROR: Folder ID missing after creating '$name' (parent $parent)."
              echo "$result" | jq .
              exit 1
            fi
            FOLDER_CACHE["$cache_key"]="$folder_id"
            echo "$folder_id"
          }

          ensure_path() {
            local parent="$1"
            local path="$2"
            if [ -z "$path" ] || [ "$path" = "." ]; then
              echo "$parent"
              return 0
            fi

            local current="$parent"
            local oldIFS="$IFS"
            IFS='/'
            read -r -a segments <<< "$path"
            IFS="$oldIFS"
            for segment in "${segments[@]}"; do
              [ -z "$segment" ] && continue
              [ "$segment" = "." ] && continue
              if [ "$segment" = ".." ]; then
                echo "ERROR: Unsupported path segment '..' in '$path'."
                exit 1
              fi
              local slug
              slug=$(sanitize_slug "$segment")
              current=$(ensure_child_folder "$current" "$segment" "$slug")
            done
            echo "$current"
          }

          echo "$MEDIA_FILES" | while IFS= read -r filepath || [ -n "$filepath" ]; do
            [ -z "$filepath" ] && continue
            if [ ! -f "$filepath" ]; then
              echo "File '$filepath' not found locally, skipping."
              continue
            fi

            target_folder_id="$DOC_ROOT_FOLDER_ID"
            relative_path="$filepath"
            relative_dir=""
            if [[ "$filepath" == documentation/* ]]; then
              relative_path="${filepath#documentation/}"
              if [[ "$relative_path" == */* ]]; then
                relative_dir="${relative_path%/*}"
                target_folder_id=$(ensure_path "$DOC_ROOT_FOLDER_ID" "$relative_dir")
              fi
            fi

            echo "Uploading $filepath"
            RESPONSE_WITH_STATUS=$(curl -s -w "\n%{http_code}" -X POST "$UPLOAD_URL" \
              -H "Authorization: Bearer $WIKI_API_TOKEN" \
              -F "mediaUpload={\"folderId\":$target_folder_id}" \
              -F "mediaUpload=@$filepath")
            HTTP_STATUS=$(echo "$RESPONSE_WITH_STATUS" | tail -n1)
            BODY=$(echo "$RESPONSE_WITH_STATUS" | sed '$d')
            if [[ "$HTTP_STATUS" =~ ^2 ]]; then
              if echo "$BODY" | jq . >/dev/null 2>&1; then
                echo "$BODY" | jq .
              else
                echo "Upload succeeded (HTTP $HTTP_STATUS) for file: $filepath"
                echo "$BODY"
              fi
            else
              echo "Upload failed (HTTP $HTTP_STATUS) for file: $filepath"
              if echo "$BODY" | jq . >/dev/null 2>&1; then
                echo "$BODY" | jq .
              else
                echo "$BODY"
              fi
              exit 1
            fi
          done
